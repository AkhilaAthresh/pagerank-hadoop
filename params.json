{"google":"","note":"Don't delete this file! It's used internally to help with page regeneration.","tagline":"My PageRank implementation with Hadoop","name":"PageRank Hadoop","body":"## Introduction\r\nThe program implments a PageRank iterative calculation on a graph with Hadoop. The input data is a directed network (not html documents). There are mainly two phases of map-reduce. The second phase will be the phase to be iterative.\r\n\r\n## Usage\r\nA bash script `pr` is provided.\r\nset `$HADOOP_INSTALL` to the directory of hadoop installation.\r\n```\r\n$ ./pr 1\r\n# run phase 1, input folder: graph_input, output folder: phase1_output\r\n$ ./pr 2 [iteration]\r\n# run phase 2 for [iterative] times, input folder: phase2_output_[i], output_folder: phase2_output_[i+1]\r\n```\r\n\r\n## About PageRank (review)\r\n**Overall**: The ranking of a page is determined by its estimated importance (determined by link structure) instead of by its content.\r\nA variation of **Eigenvector centrality**. \r\n![PageRank equation](http://upload.wikimedia.org/math/3/0/1/301ac52562803bc429fe3d8dace97b6b.png) ![](http://upload.wikimedia.org/math/0/3/1/03114db8703c7f3b642c7c43a7c5b2c0.png)\r\n\r\n**each iteration: PageRank divided by the number of outbound links L(j)**\r\n**probability distribution:** the bigger the PageRank, the more important the page\r\nd = 0.85 [introduction to PageRank](http://michaelnielsen.org/blog/lectures-on-the-google-technology-stack-1-introduction-to-pagerank/)\r\n\r\nPageRank Convergence: It may never reach convergence for loops. Ususally 10 iterations will be enough.\r\n\r\n## Computing PageRank using MapReduce\r\n#### Phase 1 - Parse documents (web pages) for links\r\nThis phase is simplified by replacing web pages with a network file.\r\n##### map\r\n```\r\nA\\tD \t(A, D)\r\nB\\tC \t(B, C)\r\nC\\tA \t(C, A)\r\nD\\tB\t(D, B)\r\nC\\tD\t(C, D)\r\n```\r\n#### reduce\r\n```\r\n(A, D)\t(A, (1.0:D))\t\r\n(B, C)\t(B, (1.0:C))\r\n(C, A)\t(C, (1.0:A,D))\r\n(D, B)\t(D, (1.0:B))\r\n(C, D)\r\n```\r\n#### notice\r\nPageRank results usually present in a probility distribution. In MapReduce program, there is no idea of number of nodes in the graph, all PageRank values are set 1.0 on initialization.\r\n\r\n#### Phase 2 - Compute PageRank (Iteratively approach)\r\n##### map\r\n```\r\n(A, (1.0:D))\t(D, 1:1)\t\r\n(B, (1.0:C))\t(C, 1:1)\r\n(C, (1.0:A,D))\t(A, 1:2)\r\n(D, (1.0:B))\t(D, 1:2)\r\n\t\t\t\t(B, 1:1)\r\n\t\t\t\t**(A, D)**\r\n\t\t\t\t**(B, C)**\r\n\t\t\t\t**(C, AD)**\r\n\t\t\t\t**(D, B)**\r\n```\r\n##### reduce\r\n```\r\n(D, 1:1)\t\t(A, (0.5:D))\r\n(C, 1:1)\t\t(B, (1.0:C))\r\n(A, 1:2)\t\t(C, (1.0:A,D))\r\n(D, 1:2)\t\t(D, (1.5:B))\r\n(B, 1:1)\r\n(A, D)\r\n(B, C)\r\n(C, AD)\r\n(D, B)\r\n```\r\n##### notice\r\nIn mapping process, the graph structure is kept such that it can be iterative.\r\nupdaing PageRank\r\n\r\n[new pr] = (dumping_value) * [old pr] + (1 - dumping_value)\r\n\r\n#### Phase 3 - Sort the nodes by PageRank\r\nSort by PageRank value.\r\n\r\n## About Iterative MapReduce\r\nThe program implements simple bash script to do iterative MapReduce. It has to restart the hadoop in each iteration.\r\n\r\n[Apache Mahout](http://mahout.apache.org/): restarting needed\r\n\r\n[PEGASUS](http://www.cs.cmu.edu/~pegasus/): restarting needed\r\n\r\n[haloop](http://code.google.com/p/haloop/): An modified version of Hadoop to support efficient iterative data processing on large commodity clusters\r\n\r\n\r\n\r\n"}